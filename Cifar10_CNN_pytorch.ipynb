{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fin_NPEXprep1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonsus/SamsungLab_Lecture/blob/master/Cifar10_CNN_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ro8HOrk4sh",
        "colab_type": "text"
      },
      "source": [
        "# Image classification 예제 (pytorch)\n",
        "## CNN 으로 CIFAR-10 분류하기 \n",
        "\n",
        "- Google Colab: 구글 드라이브 + jupyter lab + 클라우드 인스턴스 제공 (무료의 경우 GPU 가속 인스턴스는 12시간까지만 연속 사용 가능)\n",
        "\n",
        "- CIFAR10 데이터셋   \n",
        "[CIFAR10](http://www.cs.toronto.edu/~kriz/cifar.html)   \n",
        "[CIFAR10 classification records](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)\n",
        "\n",
        "- Pytorch Tensor Basic operations \n",
        "[링크](https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)  \n",
        "혹은 그냥 웬만한 튜토리얼들 좋은 자료들이 많다\n",
        "\n",
        "- CNN 구조 및 하이퍼파라미터 참조\n",
        "[솔라리스의 인공지능: TF 구현](http://solarisailab.com/archives/2325)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AsOfMXphTA5",
        "colab_type": "text"
      },
      "source": [
        "## step 0: 이 문서를 google drive에서 사본을 만든다\n",
        "> 그리고 거기에서 진행해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTsrLGH0Dl8Q",
        "colab_type": "code",
        "outputId": "e0fd0e3b-5fb9-48e5-ef75-2537a36c034a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "#런타임 유형 변경 --> GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 13 04:08:16 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n54DMSNMD9YY",
        "colab_type": "code",
        "outputId": "9ed53c14-43d5-4cab-893d-b2f877d65834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMFVNwldEotK",
        "colab_type": "code",
        "outputId": "ccfe8976-221b-48c7-f328-20e9749cd7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install scipy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.1.0 from https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arJ5b4bZEtdb",
        "colab_type": "code",
        "outputId": "9e015771-c4e2-4dcb-b3cc-fa7fabae2ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "device = torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBDBW8AaGyOU",
        "colab_type": "code",
        "outputId": "6a4c9acd-ea1b-4ecd-c167-5f8eba5bad3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "##dataset_walkthru\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# cifar10\n",
        "# http://www.cs.toronto.edu/~kriz/cifar.html\n",
        "cifar10_glance = CIFAR10('./', train=False, download=True)\n",
        "cifar10_glance._load_meta()\n",
        "idx2clsname = cifar10_glance.classes # list of classes' in the dataset\n",
        "\n",
        "anitem = cifar10_glance[808] # (PIL.Image.Image, cls_idx)\n",
        "print(anitem)\n",
        "\n",
        "anitem_pic = anitem[0]\n",
        "plt.imshow(anitem_pic)\n",
        "\n",
        "print(idx2clsname)\n",
        "print(idx2clsname[anitem[1]])\n",
        "\n",
        "#type(anitem[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "(<PIL.Image.Image image mode=RGB size=32x32 at 0x7F0CFC573668>, 7)\n",
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
            "horse\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlAj2MEzE-Lo",
        "colab_type": "code",
        "outputId": "874bbc02-5506-4311-a453-7cb46ac87d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#dataloader.py\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from pdb import set_trace\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "#tranform: PIL image --> tensors\n",
        "ds_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), #means for each channel\n",
        "                         (0.229, 0.224, 0.225))]) #stds\n",
        "\n",
        "\n",
        "\n",
        "cf10_train = CIFAR10('./', train=True, transform=ds_transform,  download=True)\n",
        "\n",
        "cf10_train._load_meta()\n",
        "idx2label = cf10_train.classes\n",
        "print(idx2label == idx2clsname) #unseen things are not knowledgeable\n",
        "print(len(cf10_train))    \n",
        "\n",
        "def collate_fn(items):\n",
        "  image_tensors, idxs  = item\n",
        "  return torch.stack(image_tensor), torch.Tensor(idxs).long()\n",
        "\n",
        "\n",
        "def get_loader(dataset, transform=None, batch_size=100, shuffle=True, drop_last=True):\n",
        "  data_loader = DataLoader(dataset=dataset, \n",
        "                           batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "  return data_loader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "True\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSPT4QK3oQJU",
        "colab_type": "text"
      },
      "source": [
        "## 모델\n",
        "### CNN 구조와 설명은 아래의 링크를 따라가보자\n",
        "[CS231n: CNN 구조(번역본)](http://aikorea.org/cs231n/convolutional-networks/\n",
        ")\n",
        "\n",
        "> 구조 세부에 대해서는 개략적으로 칠판에 설명해드릴 예정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlWrYKlfHCSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.py\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#this CNN would work for only 32x32 images with 3 channels \n",
        "class SimpleCNN(nn.Module): \n",
        "  def __init__(self, target_size):\n",
        "    super(SimpleCNN, self).__init__()\n",
        "    self.conv0 = nn.Conv2d(3,64,5, padding=1) #in-channel, #out-channel, kernel_size\n",
        "    self.conv1 = nn.Conv2d(64,64,5, padding=1)\n",
        "    self.conv2 = nn.Conv2d(64,128,3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(128,128,3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(128,128,3, padding=1)\n",
        "    self.fc0 = nn.Linear(7*7*128, 384)\n",
        "    self.fc1 = nn.Linear(384,10)\n",
        "    #self.bn0 = nn.BatchNorm1d(17**2, momentum=0.01)\n",
        "    #self.bn1 = nn.BatchNorm1d(target_size, momentum=0.01)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv0(x), kernel_size=3, stride=2, padding=1))\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=2, padding=1))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.relu(self.conv4(x))\n",
        "    #batch, chan, xaxis, yaxis = x.shape \n",
        "    x = F.dropout(F.relu(self.fc0(x.view(-1, 7*7*128))), p=0.3)\n",
        "    x = self.fc1(x)\n",
        "    prob = F.log_softmax(x, dim=0) #numerical stability issue--> use log()\n",
        "    \n",
        "    return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAmYiHq4n5wX",
        "colab_type": "text"
      },
      "source": [
        "## 학습\n",
        "### training loss < validation loss --> overfitting\n",
        "- 그러므로 그 전까지만 트레이닝 한다        \n",
        "\n",
        "### Cross Validation\n",
        "- 내가 고른 test_set 은 generalizability를 보장하는가?\n",
        "- test set을 바꿔서 다 해보자\n",
        "- test set이 정해져있다면 validation을 k-fold CV로 모델 선택에 활용이 가능하다.\n",
        "\n",
        "### more about loss\n",
        "https://ratsgo.github.io/deep%20learning/2017/09/24/loss/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh7F6DNWnPAt",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://www.researchgate.net/profile/B_Aksasse/publication/326866871/figure/fig2/AS:669601385947145@1536656819574/K-fold-cross-validation-In-addition-we-outline-an-overview-of-the-different-metrics-used.jpgg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaDUSBvcU3Ki",
        "colab_type": "code",
        "outputId": "5514f539-67e3-4505-b537-59404e73cd49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "cf10_test = CIFAR10('./', train=False, transform=ds_transform,  download=True)\n",
        "test_loader = get_loader(cf10_test, shuffle=False)\n",
        "len(test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0r5DUuFHY90",
        "colab_type": "code",
        "outputId": "139f459f-775b-4ef9-826d-b2ae8aeb0b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12513
        }
      },
      "source": [
        "#train.py\n",
        "train_loader = get_loader(cf10_train, batch_size=100) #transform=ds_transform)\n",
        "\n",
        "#1 epoch = 500 batches \n",
        "\n",
        "CNNmodel= SimpleCNN(len(idx2label)).to(device)\n",
        "\n",
        "#params to be optimized \n",
        "params = CNNmodel.parameters()\n",
        "#optimizer\n",
        "optimizer = torch.optim.Adam(params, lr=5e-4)\n",
        "\n",
        "#lr scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                       mode='max', \n",
        "                                                       factor=0.5, \n",
        "                                                       patience=2, \n",
        "                                                       verbose=True)\n",
        "#criterion\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "val_loss = 0\n",
        "train_loss = 0\n",
        "epoch = 0 \n",
        "\n",
        "prec = 0\n",
        "tprec_sum = 0\n",
        "vprec_sum = 0\n",
        "\n",
        "train_prec_avg = 0\n",
        "val_prec_avg = 0\n",
        "\n",
        "overfitcount = 0 \n",
        "\n",
        "#CNNmodel.load_state_dict(torch.load('solaris_SimpleCNN-36.pth'))\n",
        "#epoch=37\n",
        "\n",
        "while epoch < 50:  \n",
        "  tprec_sum =0 \n",
        "  vprec_sum =0\n",
        "  train_loss =0\n",
        "  val_loss =0\n",
        "  for batch, (b_img_t, b_label) in enumerate(train_loader):\n",
        "    b_img_t = b_img_t.to(device)\n",
        "    b_label = b_label.to(device)\n",
        "    #if batch<400:\n",
        "    optimizer.zero_grad()# or CNNmodel.zero_grad()\n",
        "    CNNmodel.train()\n",
        "    train_loss = 0\n",
        "    b_pred = CNNmodel(b_img_t)\n",
        "    loss = criterion(b_pred ,b_label)\n",
        "    prec = (b_pred.argmax(dim=1)==b_label).float().sum()/len(b_label)\n",
        "\n",
        "    if batch %100==1:\n",
        "      print(\"TRAIN: ep = {}, batch = {}, loss = {}, precision = {}\".format(epoch, batch, loss, prec))\n",
        "\n",
        "    loss.backward() #calculate gradients of params\n",
        "    optimizer.step() #update params based on gradients\n",
        "    train_loss += loss.item() #caution!\n",
        "    tprec_sum+=prec \n",
        "    \n",
        "    #else: # batch: 400~499\n",
        "  for vbatch,(b_img_t, b_label) in enumerate(test_loader):\n",
        "    if vbatch>=50: break\n",
        "    b_img_t = b_img_t.to(device)\n",
        "    b_label = b_label.to(device)\n",
        "\n",
        "    CNNmodel.eval() #freeze params\n",
        "    val_loss = 0\n",
        "    b_pred = CNNmodel(b_img_t)\n",
        "    prec = (b_pred.argmax(dim=1)==b_label).float().sum()/len(b_label)\n",
        "    val_loss += criterion(b_pred ,b_label).item()\n",
        "    if vbatch %10 ==1:\n",
        "      print(\"VAL: ep = {}, batch = {}, loss = {}, prec = {}\".format(epoch, batch, val_loss, prec))\n",
        "    vprec_sum += prec\n",
        "    \n",
        "  \n",
        "  train_prec_avg = tprec_sum/500\n",
        "  val_prec_avg = vprec_sum/50\n",
        "  scheduler.step(val_prec_avg)\n",
        "  \n",
        "    \n",
        "  train_loss /= 400\n",
        "  val_loss/= 400\n",
        "  \n",
        "  print('AVG: epoch: {ep},\\ntrainloss: {tl}, trainprecavg: {trp},\\nvalidloss: {vl}, validprecavg: {valp}'.format(ep=epoch, tl=train_loss, vl=val_loss, trp= train_prec_avg, valp =val_prec_avg))\n",
        "\n",
        "\n",
        "\n",
        "  #if train_prec_avg < val_prec_avg:\n",
        "  #  overfitcount+=1; print('overfitwarn:', overfitcount) \n",
        "  torch.save(CNNmodel.state_dict(), './solaris_SimpleCNN-{}.pth'.format(epoch))\n",
        "  print('saved model-epoch: ', epoch)\n",
        "  #if overfitcount>10: \n",
        "  #  break; print('end training, ep: ', epoch)\n",
        "  epoch+=1\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: ep = 0, batch = 1, loss = 4.5993547439575195, precision = 0.17999999225139618\n",
            "TRAIN: ep = 0, batch = 101, loss = 4.0462327003479, precision = 0.3700000047683716\n",
            "TRAIN: ep = 0, batch = 201, loss = 3.9025940895080566, precision = 0.3700000047683716\n",
            "TRAIN: ep = 0, batch = 301, loss = 3.65437650680542, precision = 0.5199999809265137\n",
            "TRAIN: ep = 0, batch = 401, loss = 3.4310302734375, precision = 0.6299999952316284\n",
            "VAL: ep = 0, batch = 499, loss = 3.4658446311950684, prec = 0.6399999856948853\n",
            "VAL: ep = 0, batch = 499, loss = 3.3683907985687256, prec = 0.7199999690055847\n",
            "VAL: ep = 0, batch = 499, loss = 3.4091250896453857, prec = 0.6399999856948853\n",
            "VAL: ep = 0, batch = 499, loss = 3.526055335998535, prec = 0.5999999642372131\n",
            "VAL: ep = 0, batch = 499, loss = 3.4587979316711426, prec = 0.550000011920929\n",
            "AVG: epoch: 0,\n",
            "trainloss: 0.00912075400352478, trainprecavg: 0.47808021306991577,\n",
            "validloss: 0.008623740673065185, validprecavg: 0.6000000834465027\n",
            "saved model-epoch:  0\n",
            "TRAIN: ep = 1, batch = 1, loss = 3.5107312202453613, precision = 0.5999999642372131\n",
            "TRAIN: ep = 1, batch = 101, loss = 3.4657821655273438, precision = 0.6200000047683716\n",
            "TRAIN: ep = 1, batch = 201, loss = 3.3544368743896484, precision = 0.6699999570846558\n",
            "TRAIN: ep = 1, batch = 301, loss = 3.2539284229278564, precision = 0.6800000071525574\n",
            "TRAIN: ep = 1, batch = 401, loss = 3.248915195465088, precision = 0.6200000047683716\n",
            "VAL: ep = 1, batch = 499, loss = 3.5387487411499023, prec = 0.5799999833106995\n",
            "VAL: ep = 1, batch = 499, loss = 3.3063430786132812, prec = 0.699999988079071\n",
            "VAL: ep = 1, batch = 499, loss = 3.2751407623291016, prec = 0.6899999976158142\n",
            "VAL: ep = 1, batch = 499, loss = 3.3383889198303223, prec = 0.7099999785423279\n",
            "VAL: ep = 1, batch = 499, loss = 3.400801658630371, prec = 0.6599999666213989\n",
            "AVG: epoch: 1,\n",
            "trainloss: 0.008047261834144592, trainprecavg: 0.643520176410675,\n",
            "validloss: 0.008016697764396667, validprecavg: 0.6633998155593872\n",
            "saved model-epoch:  1\n",
            "TRAIN: ep = 2, batch = 1, loss = 3.187281847000122, precision = 0.7199999690055847\n",
            "TRAIN: ep = 2, batch = 101, loss = 3.144394636154175, precision = 0.75\n",
            "TRAIN: ep = 2, batch = 201, loss = 3.227611780166626, precision = 0.6800000071525574\n",
            "TRAIN: ep = 2, batch = 301, loss = 3.3154289722442627, precision = 0.6699999570846558\n",
            "TRAIN: ep = 2, batch = 401, loss = 3.0677783489227295, precision = 0.7400000095367432\n",
            "VAL: ep = 2, batch = 499, loss = 3.123070001602173, prec = 0.7199999690055847\n",
            "VAL: ep = 2, batch = 499, loss = 3.259542942047119, prec = 0.7099999785423279\n",
            "VAL: ep = 2, batch = 499, loss = 3.0798773765563965, prec = 0.7099999785423279\n",
            "VAL: ep = 2, batch = 499, loss = 3.003288984298706, prec = 0.7299999594688416\n",
            "VAL: ep = 2, batch = 499, loss = 3.153454065322876, prec = 0.699999988079071\n",
            "AVG: epoch: 2,\n",
            "trainloss: 0.007720546722412109, trainprecavg: 0.7091801166534424,\n",
            "validloss: 0.007485992908477783, validprecavg: 0.7087998986244202\n",
            "saved model-epoch:  2\n",
            "TRAIN: ep = 3, batch = 1, loss = 3.056705951690674, precision = 0.7400000095367432\n",
            "TRAIN: ep = 3, batch = 101, loss = 3.0935051441192627, precision = 0.75\n",
            "TRAIN: ep = 3, batch = 201, loss = 3.1352286338806152, precision = 0.6699999570846558\n",
            "TRAIN: ep = 3, batch = 301, loss = 2.9966678619384766, precision = 0.7899999618530273\n",
            "TRAIN: ep = 3, batch = 401, loss = 2.9483578205108643, precision = 0.8399999737739563\n",
            "VAL: ep = 3, batch = 499, loss = 2.9959051609039307, prec = 0.7599999904632568\n",
            "VAL: ep = 3, batch = 499, loss = 3.0866270065307617, prec = 0.7199999690055847\n",
            "VAL: ep = 3, batch = 499, loss = 3.110438585281372, prec = 0.6800000071525574\n",
            "VAL: ep = 3, batch = 499, loss = 3.072524309158325, prec = 0.75\n",
            "VAL: ep = 3, batch = 499, loss = 3.1155223846435547, prec = 0.7299999594688416\n",
            "AVG: epoch: 3,\n",
            "trainloss: 0.008064245581626892, trainprecavg: 0.7503994107246399,\n",
            "validloss: 0.007996208667755127, validprecavg: 0.7392000555992126\n",
            "saved model-epoch:  3\n",
            "TRAIN: ep = 4, batch = 1, loss = 2.8498382568359375, precision = 0.8199999928474426\n",
            "TRAIN: ep = 4, batch = 101, loss = 2.9623632431030273, precision = 0.7699999809265137\n",
            "TRAIN: ep = 4, batch = 201, loss = 2.822935104370117, precision = 0.8199999928474426\n",
            "TRAIN: ep = 4, batch = 301, loss = 2.9432260990142822, precision = 0.7699999809265137\n",
            "TRAIN: ep = 4, batch = 401, loss = 3.0393781661987305, precision = 0.7699999809265137\n",
            "VAL: ep = 4, batch = 499, loss = 3.17751407623291, prec = 0.7199999690055847\n",
            "VAL: ep = 4, batch = 499, loss = 3.087977886199951, prec = 0.7299999594688416\n",
            "VAL: ep = 4, batch = 499, loss = 2.9615325927734375, prec = 0.7899999618530273\n",
            "VAL: ep = 4, batch = 499, loss = 3.052388906478882, prec = 0.7199999690055847\n",
            "VAL: ep = 4, batch = 499, loss = 3.0657687187194824, prec = 0.7799999713897705\n",
            "AVG: epoch: 4,\n",
            "trainloss: 0.0073014342784881595, trainprecavg: 0.7852997183799744,\n",
            "validloss: 0.007519991993904114, validprecavg: 0.7540000677108765\n",
            "saved model-epoch:  4\n",
            "TRAIN: ep = 5, batch = 1, loss = 2.8604865074157715, precision = 0.7999999523162842\n",
            "TRAIN: ep = 5, batch = 101, loss = 2.91033935546875, precision = 0.8299999833106995\n",
            "TRAIN: ep = 5, batch = 201, loss = 3.0028693675994873, precision = 0.7999999523162842\n",
            "TRAIN: ep = 5, batch = 301, loss = 2.898486375808716, precision = 0.8399999737739563\n",
            "TRAIN: ep = 5, batch = 401, loss = 2.8554067611694336, precision = 0.8199999928474426\n",
            "VAL: ep = 5, batch = 499, loss = 2.9687066078186035, prec = 0.7799999713897705\n",
            "VAL: ep = 5, batch = 499, loss = 3.1108663082122803, prec = 0.75\n",
            "VAL: ep = 5, batch = 499, loss = 3.096672773361206, prec = 0.8199999928474426\n",
            "VAL: ep = 5, batch = 499, loss = 2.76560115814209, prec = 0.8700000047683716\n",
            "VAL: ep = 5, batch = 499, loss = 2.987136125564575, prec = 0.7799999713897705\n",
            "AVG: epoch: 5,\n",
            "trainloss: 0.007216216921806335, trainprecavg: 0.8109593391418457,\n",
            "validloss: 0.007569139003753662, validprecavg: 0.7731999754905701\n",
            "saved model-epoch:  5\n",
            "TRAIN: ep = 6, batch = 1, loss = 2.838052988052368, precision = 0.8100000023841858\n",
            "TRAIN: ep = 6, batch = 101, loss = 2.8627490997314453, precision = 0.8299999833106995\n",
            "TRAIN: ep = 6, batch = 201, loss = 2.7790069580078125, precision = 0.9300000071525574\n",
            "TRAIN: ep = 6, batch = 301, loss = 2.9326608180999756, precision = 0.7799999713897705\n",
            "TRAIN: ep = 6, batch = 401, loss = 2.76322078704834, precision = 0.8499999642372131\n",
            "VAL: ep = 6, batch = 499, loss = 3.068901777267456, prec = 0.7400000095367432\n",
            "VAL: ep = 6, batch = 499, loss = 3.160465955734253, prec = 0.7699999809265137\n",
            "VAL: ep = 6, batch = 499, loss = 3.0647034645080566, prec = 0.8100000023841858\n",
            "VAL: ep = 6, batch = 499, loss = 3.110621452331543, prec = 0.7699999809265137\n",
            "VAL: ep = 6, batch = 499, loss = 3.0764496326446533, prec = 0.7699999809265137\n",
            "AVG: epoch: 6,\n",
            "trainloss: 0.00713229775428772, trainprecavg: 0.8331791162490845,\n",
            "validloss: 0.007341877818107605, validprecavg: 0.777400016784668\n",
            "saved model-epoch:  6\n",
            "TRAIN: ep = 7, batch = 1, loss = 2.8115484714508057, precision = 0.8499999642372131\n",
            "TRAIN: ep = 7, batch = 101, loss = 2.6821017265319824, precision = 0.8399999737739563\n",
            "TRAIN: ep = 7, batch = 201, loss = 2.6927108764648438, precision = 0.8899999856948853\n",
            "TRAIN: ep = 7, batch = 301, loss = 2.812370538711548, precision = 0.8299999833106995\n",
            "TRAIN: ep = 7, batch = 401, loss = 2.7715859413146973, precision = 0.8799999952316284\n",
            "VAL: ep = 7, batch = 499, loss = 2.990572452545166, prec = 0.7400000095367432\n",
            "VAL: ep = 7, batch = 499, loss = 3.1767101287841797, prec = 0.7699999809265137\n",
            "VAL: ep = 7, batch = 499, loss = 2.9590673446655273, prec = 0.7999999523162842\n",
            "VAL: ep = 7, batch = 499, loss = 3.0576159954071045, prec = 0.7199999690055847\n",
            "VAL: ep = 7, batch = 499, loss = 2.708038568496704, prec = 0.8799999952316284\n",
            "AVG: epoch: 7,\n",
            "trainloss: 0.006872553825378418, trainprecavg: 0.8539995551109314,\n",
            "validloss: 0.007766799926757812, validprecavg: 0.7821999192237854\n",
            "saved model-epoch:  7\n",
            "TRAIN: ep = 8, batch = 1, loss = 2.6802353858947754, precision = 0.8700000047683716\n",
            "TRAIN: ep = 8, batch = 101, loss = 2.7241644859313965, precision = 0.9300000071525574\n",
            "TRAIN: ep = 8, batch = 201, loss = 2.705491065979004, precision = 0.8999999761581421\n",
            "TRAIN: ep = 8, batch = 301, loss = 2.7436113357543945, precision = 0.8499999642372131\n",
            "TRAIN: ep = 8, batch = 401, loss = 2.7391490936279297, precision = 0.8700000047683716\n",
            "VAL: ep = 8, batch = 499, loss = 2.878329277038574, prec = 0.7899999618530273\n",
            "VAL: ep = 8, batch = 499, loss = 2.9578135013580322, prec = 0.7899999618530273\n",
            "VAL: ep = 8, batch = 499, loss = 2.9228625297546387, prec = 0.8199999928474426\n",
            "VAL: ep = 8, batch = 499, loss = 3.234067916870117, prec = 0.6800000071525574\n",
            "VAL: ep = 8, batch = 499, loss = 2.9217331409454346, prec = 0.7699999809265137\n",
            "AVG: epoch: 8,\n",
            "trainloss: 0.00673611044883728, trainprecavg: 0.8790003657341003,\n",
            "validloss: 0.0073088926076889035, validprecavg: 0.7768000364303589\n",
            "saved model-epoch:  8\n",
            "TRAIN: ep = 9, batch = 1, loss = 2.6052303314208984, precision = 0.9300000071525574\n",
            "TRAIN: ep = 9, batch = 101, loss = 2.5809812545776367, precision = 0.8799999952316284\n",
            "TRAIN: ep = 9, batch = 201, loss = 2.5359246730804443, precision = 0.9399999976158142\n",
            "TRAIN: ep = 9, batch = 301, loss = 2.6764392852783203, precision = 0.8899999856948853\n",
            "TRAIN: ep = 9, batch = 401, loss = 2.6159350872039795, precision = 0.9399999976158142\n",
            "VAL: ep = 9, batch = 499, loss = 2.973578691482544, prec = 0.7599999904632568\n",
            "VAL: ep = 9, batch = 499, loss = 3.176372766494751, prec = 0.7999999523162842\n",
            "VAL: ep = 9, batch = 499, loss = 2.972011089324951, prec = 0.85999995470047\n",
            "VAL: ep = 9, batch = 499, loss = 2.995847463607788, prec = 0.7599999904632568\n",
            "VAL: ep = 9, batch = 499, loss = 2.973564863204956, prec = 0.8399999737739563\n",
            "AVG: epoch: 9,\n",
            "trainloss: 0.006583874225616455, trainprecavg: 0.897380530834198,\n",
            "validloss: 0.007820792198181152, validprecavg: 0.7974000573158264\n",
            "saved model-epoch:  9\n",
            "TRAIN: ep = 10, batch = 1, loss = 2.537775993347168, precision = 0.9300000071525574\n",
            "TRAIN: ep = 10, batch = 101, loss = 2.6230814456939697, precision = 0.85999995470047\n",
            "TRAIN: ep = 10, batch = 201, loss = 2.6107845306396484, precision = 0.9199999570846558\n",
            "TRAIN: ep = 10, batch = 301, loss = 2.652454137802124, precision = 0.8999999761581421\n",
            "TRAIN: ep = 10, batch = 401, loss = 2.619148015975952, precision = 0.8799999952316284\n",
            "VAL: ep = 10, batch = 499, loss = 3.2372982501983643, prec = 0.7199999690055847\n",
            "VAL: ep = 10, batch = 499, loss = 3.052245855331421, prec = 0.7699999809265137\n",
            "VAL: ep = 10, batch = 499, loss = 3.2199954986572266, prec = 0.7799999713897705\n",
            "VAL: ep = 10, batch = 499, loss = 3.1979002952575684, prec = 0.75\n",
            "VAL: ep = 10, batch = 499, loss = 3.300715446472168, prec = 0.7400000095367432\n",
            "AVG: epoch: 10,\n",
            "trainloss: 0.006504069566726684, trainprecavg: 0.9193811416625977,\n",
            "validloss: 0.007423034906387329, validprecavg: 0.7927999496459961\n",
            "saved model-epoch:  10\n",
            "TRAIN: ep = 11, batch = 1, loss = 2.494358777999878, precision = 0.9699999690055847\n",
            "TRAIN: ep = 11, batch = 101, loss = 2.5441603660583496, precision = 0.8899999856948853\n",
            "TRAIN: ep = 11, batch = 201, loss = 2.53926157951355, precision = 0.9699999690055847\n",
            "TRAIN: ep = 11, batch = 301, loss = 2.5804877281188965, precision = 0.9399999976158142\n",
            "TRAIN: ep = 11, batch = 401, loss = 2.657698631286621, precision = 0.8799999952316284\n",
            "VAL: ep = 11, batch = 499, loss = 3.3380956649780273, prec = 0.7199999690055847\n",
            "VAL: ep = 11, batch = 499, loss = 2.9262335300445557, prec = 0.8299999833106995\n",
            "VAL: ep = 11, batch = 499, loss = 2.9240562915802, prec = 0.7899999618530273\n",
            "VAL: ep = 11, batch = 499, loss = 3.0043017864227295, prec = 0.7899999618530273\n",
            "VAL: ep = 11, batch = 499, loss = 3.1586215496063232, prec = 0.7699999809265137\n",
            "AVG: epoch: 11,\n",
            "trainloss: 0.006169414520263672, trainprecavg: 0.936501145362854,\n",
            "validloss: 0.007037814259529113, validprecavg: 0.7963999509811401\n",
            "saved model-epoch:  11\n",
            "TRAIN: ep = 12, batch = 1, loss = 2.500164270401001, precision = 0.9599999785423279\n",
            "TRAIN: ep = 12, batch = 101, loss = 2.5910019874572754, precision = 0.9399999976158142\n",
            "TRAIN: ep = 12, batch = 201, loss = 2.5400397777557373, precision = 0.9399999976158142\n",
            "TRAIN: ep = 12, batch = 301, loss = 2.504774332046509, precision = 0.9399999976158142\n",
            "TRAIN: ep = 12, batch = 401, loss = 2.4743034839630127, precision = 0.9799999594688416\n",
            "VAL: ep = 12, batch = 499, loss = 3.4054551124572754, prec = 0.75\n",
            "VAL: ep = 12, batch = 499, loss = 2.992619514465332, prec = 0.8100000023841858\n",
            "VAL: ep = 12, batch = 499, loss = 2.958120107650757, prec = 0.8100000023841858\n",
            "VAL: ep = 12, batch = 499, loss = 3.411928415298462, prec = 0.7299999594688416\n",
            "VAL: ep = 12, batch = 499, loss = 2.8738772869110107, prec = 0.8399999737739563\n",
            "Epoch    12: reducing learning rate of group 0 to 2.5000e-04.\n",
            "AVG: epoch: 12,\n",
            "trainloss: 0.006094636917114257, trainprecavg: 0.9491211175918579,\n",
            "validloss: 0.008160039782524109, validprecavg: 0.793199896812439\n",
            "saved model-epoch:  12\n",
            "TRAIN: ep = 13, batch = 1, loss = 2.4346742630004883, precision = 0.9699999690055847\n",
            "TRAIN: ep = 13, batch = 101, loss = 2.408066987991333, precision = 1.0\n",
            "TRAIN: ep = 13, batch = 201, loss = 2.4520230293273926, precision = 0.9799999594688416\n",
            "TRAIN: ep = 13, batch = 301, loss = 2.4250705242156982, precision = 0.9599999785423279\n",
            "TRAIN: ep = 13, batch = 401, loss = 2.3882265090942383, precision = 1.0\n",
            "VAL: ep = 13, batch = 499, loss = 3.1622040271759033, prec = 0.8299999833106995\n",
            "VAL: ep = 13, batch = 499, loss = 3.0480782985687256, prec = 0.85999995470047\n",
            "VAL: ep = 13, batch = 499, loss = 3.1498706340789795, prec = 0.7699999809265137\n",
            "VAL: ep = 13, batch = 499, loss = 3.523259162902832, prec = 0.7599999904632568\n",
            "VAL: ep = 13, batch = 499, loss = 2.9840807914733887, prec = 0.8700000047683716\n",
            "AVG: epoch: 13,\n",
            "trainloss: 0.00604669451713562, trainprecavg: 0.9821403622627258,\n",
            "validloss: 0.00778388261795044, validprecavg: 0.8050001263618469\n",
            "saved model-epoch:  13\n",
            "TRAIN: ep = 14, batch = 1, loss = 2.3868987560272217, precision = 0.9799999594688416\n",
            "TRAIN: ep = 14, batch = 101, loss = 2.3851449489593506, precision = 1.0\n",
            "TRAIN: ep = 14, batch = 201, loss = 2.417396306991577, precision = 1.0\n",
            "TRAIN: ep = 14, batch = 301, loss = 2.3770785331726074, precision = 0.9899999499320984\n",
            "TRAIN: ep = 14, batch = 401, loss = 2.369135856628418, precision = 1.0\n",
            "VAL: ep = 14, batch = 499, loss = 3.4616241455078125, prec = 0.7999999523162842\n",
            "VAL: ep = 14, batch = 499, loss = 3.0827383995056152, prec = 0.8700000047683716\n",
            "VAL: ep = 14, batch = 499, loss = 3.1548337936401367, prec = 0.8299999833106995\n",
            "VAL: ep = 14, batch = 499, loss = 3.297619342803955, prec = 0.7699999809265137\n",
            "VAL: ep = 14, batch = 499, loss = 3.5253164768218994, prec = 0.7799999713897705\n",
            "AVG: epoch: 14,\n",
            "trainloss: 0.006160091161727905, trainprecavg: 0.9909796714782715,\n",
            "validloss: 0.008153835535049439, validprecavg: 0.7966001033782959\n",
            "saved model-epoch:  14\n",
            "TRAIN: ep = 15, batch = 1, loss = 2.368487596511841, precision = 1.0\n",
            "TRAIN: ep = 15, batch = 101, loss = 2.3836638927459717, precision = 1.0\n",
            "TRAIN: ep = 15, batch = 201, loss = 2.3640520572662354, precision = 0.9899999499320984\n",
            "TRAIN: ep = 15, batch = 301, loss = 2.3878512382507324, precision = 1.0\n",
            "TRAIN: ep = 15, batch = 401, loss = 2.367892026901245, precision = 0.9899999499320984\n",
            "VAL: ep = 15, batch = 499, loss = 3.8178112506866455, prec = 0.7699999809265137\n",
            "VAL: ep = 15, batch = 499, loss = 3.321657419204712, prec = 0.7899999618530273\n",
            "VAL: ep = 15, batch = 499, loss = 2.985971450805664, prec = 0.8299999833106995\n",
            "VAL: ep = 15, batch = 499, loss = 3.625467300415039, prec = 0.7999999523162842\n",
            "VAL: ep = 15, batch = 499, loss = 3.6161768436431885, prec = 0.8100000023841858\n",
            "AVG: epoch: 15,\n",
            "trainloss: 0.006034834384918213, trainprecavg: 0.9943794012069702,\n",
            "validloss: 0.008613486289978028, validprecavg: 0.8080000281333923\n",
            "saved model-epoch:  15\n",
            "TRAIN: ep = 16, batch = 1, loss = 2.3718695640563965, precision = 1.0\n",
            "TRAIN: ep = 16, batch = 101, loss = 2.3718926906585693, precision = 1.0\n",
            "TRAIN: ep = 16, batch = 201, loss = 2.3748221397399902, precision = 1.0\n",
            "TRAIN: ep = 16, batch = 301, loss = 2.3945541381835938, precision = 0.9799999594688416\n",
            "TRAIN: ep = 16, batch = 401, loss = 2.3921959400177, precision = 1.0\n",
            "VAL: ep = 16, batch = 499, loss = 3.024125576019287, prec = 0.8700000047683716\n",
            "VAL: ep = 16, batch = 499, loss = 3.0232086181640625, prec = 0.8100000023841858\n",
            "VAL: ep = 16, batch = 499, loss = 3.461806058883667, prec = 0.8299999833106995\n",
            "VAL: ep = 16, batch = 499, loss = 3.0651321411132812, prec = 0.8100000023841858\n",
            "VAL: ep = 16, batch = 499, loss = 3.530738115310669, prec = 0.8199999928474426\n",
            "AVG: epoch: 16,\n",
            "trainloss: 0.005912057757377624, trainprecavg: 0.996059238910675,\n",
            "validloss: 0.008145062923431397, validprecavg: 0.7992000579833984\n",
            "saved model-epoch:  16\n",
            "TRAIN: ep = 17, batch = 1, loss = 2.3580641746520996, precision = 1.0\n",
            "TRAIN: ep = 17, batch = 101, loss = 2.361015796661377, precision = 1.0\n",
            "TRAIN: ep = 17, batch = 201, loss = 2.415584087371826, precision = 1.0\n",
            "TRAIN: ep = 17, batch = 301, loss = 2.416790008544922, precision = 0.9899999499320984\n",
            "TRAIN: ep = 17, batch = 401, loss = 2.354951858520508, precision = 1.0\n",
            "VAL: ep = 17, batch = 499, loss = 3.741711378097534, prec = 0.8499999642372131\n",
            "VAL: ep = 17, batch = 499, loss = 3.6506333351135254, prec = 0.7899999618530273\n",
            "VAL: ep = 17, batch = 499, loss = 3.3299472332000732, prec = 0.8499999642372131\n",
            "VAL: ep = 17, batch = 499, loss = 3.6530203819274902, prec = 0.7799999713897705\n",
            "VAL: ep = 17, batch = 499, loss = 3.4185292720794678, prec = 0.8399999737739563\n",
            "AVG: epoch: 17,\n",
            "trainloss: 0.005886072516441345, trainprecavg: 0.996239185333252,\n",
            "validloss: 0.011267801523208618, validprecavg: 0.8024000525474548\n",
            "saved model-epoch:  17\n",
            "TRAIN: ep = 18, batch = 1, loss = 2.3663082122802734, precision = 0.9899999499320984\n",
            "TRAIN: ep = 18, batch = 101, loss = 2.3917603492736816, precision = 1.0\n",
            "TRAIN: ep = 18, batch = 201, loss = 2.3470566272735596, precision = 1.0\n",
            "TRAIN: ep = 18, batch = 301, loss = 2.3465347290039062, precision = 1.0\n",
            "TRAIN: ep = 18, batch = 401, loss = 2.395688056945801, precision = 1.0\n",
            "VAL: ep = 18, batch = 499, loss = 3.964481830596924, prec = 0.7400000095367432\n",
            "VAL: ep = 18, batch = 499, loss = 3.569246292114258, prec = 0.7999999523162842\n",
            "VAL: ep = 18, batch = 499, loss = 3.600492477416992, prec = 0.8100000023841858\n",
            "VAL: ep = 18, batch = 499, loss = 3.534041404724121, prec = 0.7599999904632568\n",
            "VAL: ep = 18, batch = 499, loss = 3.6123111248016357, prec = 0.8299999833106995\n",
            "AVG: epoch: 18,\n",
            "trainloss: 0.0060966753959655765, trainprecavg: 0.9929800033569336,\n",
            "validloss: 0.009044178128242493, validprecavg: 0.8086000680923462\n",
            "saved model-epoch:  18\n",
            "TRAIN: ep = 19, batch = 1, loss = 2.38081431388855, precision = 1.0\n",
            "TRAIN: ep = 19, batch = 101, loss = 2.394911050796509, precision = 1.0\n",
            "TRAIN: ep = 19, batch = 201, loss = 2.3636248111724854, precision = 1.0\n",
            "TRAIN: ep = 19, batch = 301, loss = 2.3467533588409424, precision = 1.0\n",
            "TRAIN: ep = 19, batch = 401, loss = 2.344693183898926, precision = 1.0\n",
            "VAL: ep = 19, batch = 499, loss = 3.2035863399505615, prec = 0.8199999928474426\n",
            "VAL: ep = 19, batch = 499, loss = 3.592212200164795, prec = 0.7899999618530273\n",
            "VAL: ep = 19, batch = 499, loss = 4.258077621459961, prec = 0.7699999809265137\n",
            "VAL: ep = 19, batch = 499, loss = 3.5195369720458984, prec = 0.8100000023841858\n",
            "VAL: ep = 19, batch = 499, loss = 3.0254404544830322, prec = 0.85999995470047\n",
            "AVG: epoch: 19,\n",
            "trainloss: 0.005963159203529358, trainprecavg: 0.9975192546844482,\n",
            "validloss: 0.009006555676460266, validprecavg: 0.7952001690864563\n",
            "saved model-epoch:  19\n",
            "TRAIN: ep = 20, batch = 1, loss = 2.3768198490142822, precision = 0.9899999499320984\n",
            "TRAIN: ep = 20, batch = 101, loss = 2.367800235748291, precision = 1.0\n",
            "TRAIN: ep = 20, batch = 201, loss = 2.3679959774017334, precision = 1.0\n",
            "TRAIN: ep = 20, batch = 301, loss = 2.3907220363616943, precision = 1.0\n",
            "TRAIN: ep = 20, batch = 401, loss = 2.369378089904785, precision = 1.0\n",
            "VAL: ep = 20, batch = 499, loss = 3.4221158027648926, prec = 0.8100000023841858\n",
            "VAL: ep = 20, batch = 499, loss = 3.4668807983398438, prec = 0.8399999737739563\n",
            "VAL: ep = 20, batch = 499, loss = 3.582761287689209, prec = 0.7899999618530273\n",
            "VAL: ep = 20, batch = 499, loss = 4.233173370361328, prec = 0.7099999785423279\n",
            "VAL: ep = 20, batch = 499, loss = 3.188899278640747, prec = 0.8199999928474426\n",
            "AVG: epoch: 20,\n",
            "trainloss: 0.0058752518892288205, trainprecavg: 0.9971995949745178,\n",
            "validloss: 0.010816861391067505, validprecavg: 0.7981998920440674\n",
            "saved model-epoch:  20\n",
            "TRAIN: ep = 21, batch = 1, loss = 2.422687292098999, precision = 0.9799999594688416\n",
            "TRAIN: ep = 21, batch = 101, loss = 2.3970656394958496, precision = 0.9899999499320984\n",
            "TRAIN: ep = 21, batch = 201, loss = 2.3510165214538574, precision = 1.0\n",
            "TRAIN: ep = 21, batch = 301, loss = 2.4743247032165527, precision = 0.9799999594688416\n",
            "TRAIN: ep = 21, batch = 401, loss = 2.3859424591064453, precision = 1.0\n",
            "VAL: ep = 21, batch = 499, loss = 4.679450035095215, prec = 0.7999999523162842\n",
            "VAL: ep = 21, batch = 499, loss = 3.4862072467803955, prec = 0.8299999833106995\n",
            "VAL: ep = 21, batch = 499, loss = 3.8943004608154297, prec = 0.8199999928474426\n",
            "VAL: ep = 21, batch = 499, loss = 3.9857285022735596, prec = 0.7699999809265137\n",
            "VAL: ep = 21, batch = 499, loss = 3.726814031600952, prec = 0.8199999928474426\n",
            "Epoch    21: reducing learning rate of group 0 to 1.2500e-04.\n",
            "AVG: epoch: 21,\n",
            "trainloss: 0.0059111124277114865, trainprecavg: 0.9942997694015503,\n",
            "validloss: 0.00916186809539795, validprecavg: 0.8040000200271606\n",
            "saved model-epoch:  21\n",
            "TRAIN: ep = 22, batch = 1, loss = 2.409534215927124, precision = 0.9799999594688416\n",
            "TRAIN: ep = 22, batch = 101, loss = 2.359160900115967, precision = 1.0\n",
            "TRAIN: ep = 22, batch = 201, loss = 2.3567090034484863, precision = 1.0\n",
            "TRAIN: ep = 22, batch = 301, loss = 2.395860195159912, precision = 1.0\n",
            "TRAIN: ep = 22, batch = 401, loss = 2.3503541946411133, precision = 1.0\n",
            "VAL: ep = 22, batch = 499, loss = 4.224031925201416, prec = 0.7899999618530273\n",
            "VAL: ep = 22, batch = 499, loss = 4.621551036834717, prec = 0.7599999904632568\n",
            "VAL: ep = 22, batch = 499, loss = 3.5333526134490967, prec = 0.8299999833106995\n",
            "VAL: ep = 22, batch = 499, loss = 3.0889339447021484, prec = 0.8700000047683716\n",
            "VAL: ep = 22, batch = 499, loss = 3.747190475463867, prec = 0.75\n",
            "AVG: epoch: 22,\n",
            "trainloss: 0.005836251974105835, trainprecavg: 0.9993799924850464,\n",
            "validloss: 0.01021257758140564, validprecavg: 0.813200056552887\n",
            "saved model-epoch:  22\n",
            "TRAIN: ep = 23, batch = 1, loss = 2.3607804775238037, precision = 1.0\n",
            "TRAIN: ep = 23, batch = 101, loss = 2.3441388607025146, precision = 1.0\n",
            "TRAIN: ep = 23, batch = 201, loss = 2.3542778491973877, precision = 1.0\n",
            "TRAIN: ep = 23, batch = 301, loss = 2.3398349285125732, precision = 1.0\n",
            "TRAIN: ep = 23, batch = 401, loss = 2.3510324954986572, precision = 1.0\n",
            "VAL: ep = 23, batch = 499, loss = 3.594571828842163, prec = 0.8199999928474426\n",
            "VAL: ep = 23, batch = 499, loss = 4.086535930633545, prec = 0.7799999713897705\n",
            "VAL: ep = 23, batch = 499, loss = 4.042731761932373, prec = 0.8299999833106995\n",
            "VAL: ep = 23, batch = 499, loss = 4.1529622077941895, prec = 0.8299999833106995\n",
            "VAL: ep = 23, batch = 499, loss = 3.589221715927124, prec = 0.8299999833106995\n",
            "AVG: epoch: 23,\n",
            "trainloss: 0.005901495814323425, trainprecavg: 0.9997199177742004,\n",
            "validloss: 0.010518666505813599, validprecavg: 0.8224000334739685\n",
            "saved model-epoch:  23\n",
            "TRAIN: ep = 24, batch = 1, loss = 2.345082998275757, precision = 1.0\n",
            "TRAIN: ep = 24, batch = 101, loss = 2.351670265197754, precision = 1.0\n",
            "TRAIN: ep = 24, batch = 201, loss = 2.330443859100342, precision = 1.0\n",
            "TRAIN: ep = 24, batch = 301, loss = 2.3338587284088135, precision = 1.0\n",
            "TRAIN: ep = 24, batch = 401, loss = 2.3397891521453857, precision = 1.0\n",
            "VAL: ep = 24, batch = 499, loss = 3.834085702896118, prec = 0.7999999523162842\n",
            "VAL: ep = 24, batch = 499, loss = 3.742105722427368, prec = 0.8299999833106995\n",
            "VAL: ep = 24, batch = 499, loss = 4.28410005569458, prec = 0.8199999928474426\n",
            "VAL: ep = 24, batch = 499, loss = 3.675812005996704, prec = 0.7999999523162842\n",
            "VAL: ep = 24, batch = 499, loss = 4.917667388916016, prec = 0.7299999594688416\n",
            "AVG: epoch: 24,\n",
            "trainloss: 0.0058495402336120605, trainprecavg: 0.9998199939727783,\n",
            "validloss: 0.009005796909332276, validprecavg: 0.8079999089241028\n",
            "saved model-epoch:  24\n",
            "TRAIN: ep = 25, batch = 1, loss = 2.3510031700134277, precision = 1.0\n",
            "TRAIN: ep = 25, batch = 101, loss = 2.373277187347412, precision = 1.0\n",
            "TRAIN: ep = 25, batch = 201, loss = 2.36484432220459, precision = 1.0\n",
            "TRAIN: ep = 25, batch = 301, loss = 2.3481838703155518, precision = 1.0\n",
            "TRAIN: ep = 25, batch = 401, loss = 2.331615924835205, precision = 1.0\n",
            "VAL: ep = 25, batch = 499, loss = 3.817268133163452, prec = 0.7999999523162842\n",
            "VAL: ep = 25, batch = 499, loss = 3.7354071140289307, prec = 0.75\n",
            "VAL: ep = 25, batch = 499, loss = 4.368781566619873, prec = 0.7599999904632568\n",
            "VAL: ep = 25, batch = 499, loss = 3.274376153945923, prec = 0.8799999952316284\n",
            "VAL: ep = 25, batch = 499, loss = 3.7983148097991943, prec = 0.8499999642372131\n",
            "AVG: epoch: 25,\n",
            "trainloss: 0.005892366170883179, trainprecavg: 0.9998600482940674,\n",
            "validloss: 0.011247684955596924, validprecavg: 0.8180000185966492\n",
            "saved model-epoch:  25\n",
            "TRAIN: ep = 26, batch = 1, loss = 2.3838999271392822, precision = 1.0\n",
            "TRAIN: ep = 26, batch = 101, loss = 2.372945785522461, precision = 1.0\n",
            "TRAIN: ep = 26, batch = 201, loss = 2.373326301574707, precision = 1.0\n",
            "TRAIN: ep = 26, batch = 301, loss = 2.328705072402954, precision = 1.0\n",
            "TRAIN: ep = 26, batch = 401, loss = 2.351423501968384, precision = 1.0\n",
            "VAL: ep = 26, batch = 499, loss = 4.006181716918945, prec = 0.8199999928474426\n",
            "VAL: ep = 26, batch = 499, loss = 4.287423133850098, prec = 0.7699999809265137\n",
            "VAL: ep = 26, batch = 499, loss = 3.855026960372925, prec = 0.7999999523162842\n",
            "VAL: ep = 26, batch = 499, loss = 3.2197659015655518, prec = 0.85999995470047\n",
            "VAL: ep = 26, batch = 499, loss = 4.811194896697998, prec = 0.7599999904632568\n",
            "Epoch    26: reducing learning rate of group 0 to 6.2500e-05.\n",
            "AVG: epoch: 26,\n",
            "trainloss: 0.006000828742980957, trainprecavg: 0.9998600482940674,\n",
            "validloss: 0.012083128690719605, validprecavg: 0.8068000674247742\n",
            "saved model-epoch:  26\n",
            "TRAIN: ep = 27, batch = 1, loss = 2.3331234455108643, precision = 1.0\n",
            "TRAIN: ep = 27, batch = 101, loss = 2.3248178958892822, precision = 1.0\n",
            "TRAIN: ep = 27, batch = 201, loss = 2.3375494480133057, precision = 1.0\n",
            "TRAIN: ep = 27, batch = 301, loss = 2.3786396980285645, precision = 1.0\n",
            "TRAIN: ep = 27, batch = 401, loss = 2.3551292419433594, precision = 1.0\n",
            "VAL: ep = 27, batch = 499, loss = 3.119645118713379, prec = 0.8899999856948853\n",
            "VAL: ep = 27, batch = 499, loss = 4.286252975463867, prec = 0.7699999809265137\n",
            "VAL: ep = 27, batch = 499, loss = 4.0985307693481445, prec = 0.8399999737739563\n",
            "VAL: ep = 27, batch = 499, loss = 4.366218090057373, prec = 0.7599999904632568\n",
            "VAL: ep = 27, batch = 499, loss = 3.8255460262298584, prec = 0.7999999523162842\n",
            "AVG: epoch: 27,\n",
            "trainloss: 0.005876855254173279, trainprecavg: 0.9998599290847778,\n",
            "validloss: 0.011793634891510009, validprecavg: 0.8079999089241028\n",
            "saved model-epoch:  27\n",
            "TRAIN: ep = 28, batch = 1, loss = 2.371131181716919, precision = 1.0\n",
            "TRAIN: ep = 28, batch = 101, loss = 2.3651821613311768, precision = 1.0\n",
            "TRAIN: ep = 28, batch = 201, loss = 2.3329899311065674, precision = 1.0\n",
            "TRAIN: ep = 28, batch = 301, loss = 2.360734462738037, precision = 1.0\n",
            "TRAIN: ep = 28, batch = 401, loss = 2.364971876144409, precision = 1.0\n",
            "VAL: ep = 28, batch = 499, loss = 3.498483180999756, prec = 0.8399999737739563\n",
            "VAL: ep = 28, batch = 499, loss = 3.609544038772583, prec = 0.8399999737739563\n",
            "VAL: ep = 28, batch = 499, loss = 4.171546459197998, prec = 0.7699999809265137\n",
            "VAL: ep = 28, batch = 499, loss = 4.6191301345825195, prec = 0.7799999713897705\n",
            "VAL: ep = 28, batch = 499, loss = 4.020586013793945, prec = 0.85999995470047\n",
            "AVG: epoch: 28,\n",
            "trainloss: 0.0058318859338760375, trainprecavg: 0.9998199939727783,\n",
            "validloss: 0.012220443487167358, validprecavg: 0.8122000694274902\n",
            "saved model-epoch:  28\n",
            "TRAIN: ep = 29, batch = 1, loss = 2.3798418045043945, precision = 1.0\n",
            "TRAIN: ep = 29, batch = 101, loss = 2.3613176345825195, precision = 1.0\n",
            "TRAIN: ep = 29, batch = 201, loss = 2.3303122520446777, precision = 1.0\n",
            "TRAIN: ep = 29, batch = 301, loss = 2.326606512069702, precision = 1.0\n",
            "TRAIN: ep = 29, batch = 401, loss = 2.3425824642181396, precision = 1.0\n",
            "VAL: ep = 29, batch = 499, loss = 4.904041290283203, prec = 0.7599999904632568\n",
            "VAL: ep = 29, batch = 499, loss = 4.084670543670654, prec = 0.7899999618530273\n",
            "VAL: ep = 29, batch = 499, loss = 3.539539098739624, prec = 0.8499999642372131\n",
            "VAL: ep = 29, batch = 499, loss = 4.682090759277344, prec = 0.7699999809265137\n",
            "VAL: ep = 29, batch = 499, loss = 4.256039142608643, prec = 0.8100000023841858\n",
            "Epoch    29: reducing learning rate of group 0 to 3.1250e-05.\n",
            "AVG: epoch: 29,\n",
            "trainloss: 0.005850899815559387, trainprecavg: 0.9999399781227112,\n",
            "validloss: 0.011684314012527466, validprecavg: 0.818600058555603\n",
            "saved model-epoch:  29\n",
            "TRAIN: ep = 30, batch = 1, loss = 2.367065191268921, precision = 1.0\n",
            "TRAIN: ep = 30, batch = 101, loss = 2.321035861968994, precision = 1.0\n",
            "TRAIN: ep = 30, batch = 201, loss = 2.332900047302246, precision = 1.0\n",
            "TRAIN: ep = 30, batch = 301, loss = 2.3692243099212646, precision = 1.0\n",
            "TRAIN: ep = 30, batch = 401, loss = 2.3594963550567627, precision = 1.0\n",
            "VAL: ep = 30, batch = 499, loss = 4.667022228240967, prec = 0.7999999523162842\n",
            "VAL: ep = 30, batch = 499, loss = 3.6960668563842773, prec = 0.7999999523162842\n",
            "VAL: ep = 30, batch = 499, loss = 4.344336986541748, prec = 0.7999999523162842\n",
            "VAL: ep = 30, batch = 499, loss = 4.110800743103027, prec = 0.7899999618530273\n",
            "VAL: ep = 30, batch = 499, loss = 4.600565433502197, prec = 0.8499999642372131\n",
            "AVG: epoch: 30,\n",
            "trainloss: 0.0059895920753479, trainprecavg: 0.9999200105667114,\n",
            "validloss: 0.009986678957939149, validprecavg: 0.8109999895095825\n",
            "saved model-epoch:  30\n",
            "TRAIN: ep = 31, batch = 1, loss = 2.3688998222351074, precision = 1.0\n",
            "TRAIN: ep = 31, batch = 101, loss = 2.376657485961914, precision = 1.0\n",
            "TRAIN: ep = 31, batch = 201, loss = 2.3575737476348877, precision = 1.0\n",
            "TRAIN: ep = 31, batch = 301, loss = 2.354562282562256, precision = 1.0\n",
            "TRAIN: ep = 31, batch = 401, loss = 2.342989444732666, precision = 1.0\n",
            "VAL: ep = 31, batch = 499, loss = 4.007116794586182, prec = 0.7999999523162842\n",
            "VAL: ep = 31, batch = 499, loss = 3.5092906951904297, prec = 0.8700000047683716\n",
            "VAL: ep = 31, batch = 499, loss = 4.993246555328369, prec = 0.75\n",
            "VAL: ep = 31, batch = 499, loss = 4.417771816253662, prec = 0.8499999642372131\n",
            "VAL: ep = 31, batch = 499, loss = 4.460986614227295, prec = 0.7599999904632568\n",
            "AVG: epoch: 31,\n",
            "trainloss: 0.005888971090316772, trainprecavg: 1.0,\n",
            "validloss: 0.012729679346084594, validprecavg: 0.8123998641967773\n",
            "saved model-epoch:  31\n",
            "TRAIN: ep = 32, batch = 1, loss = 2.3344593048095703, precision = 1.0\n",
            "TRAIN: ep = 32, batch = 101, loss = 2.3734335899353027, precision = 1.0\n",
            "TRAIN: ep = 32, batch = 201, loss = 2.3527848720550537, precision = 1.0\n",
            "TRAIN: ep = 32, batch = 301, loss = 2.326162099838257, precision = 1.0\n",
            "TRAIN: ep = 32, batch = 401, loss = 2.3462142944335938, precision = 1.0\n",
            "VAL: ep = 32, batch = 499, loss = 4.801944732666016, prec = 0.7999999523162842\n",
            "VAL: ep = 32, batch = 499, loss = 3.3860695362091064, prec = 0.8700000047683716\n",
            "VAL: ep = 32, batch = 499, loss = 3.7176554203033447, prec = 0.8399999737739563\n",
            "VAL: ep = 32, batch = 499, loss = 3.936352491378784, prec = 0.8499999642372131\n",
            "VAL: ep = 32, batch = 499, loss = 5.419618129730225, prec = 0.7699999809265137\n",
            "Epoch    32: reducing learning rate of group 0 to 1.5625e-05.\n",
            "AVG: epoch: 32,\n",
            "trainloss: 0.005895710587501526, trainprecavg: 0.9999600052833557,\n",
            "validloss: 0.011124565601348876, validprecavg: 0.812999963760376\n",
            "saved model-epoch:  32\n",
            "TRAIN: ep = 33, batch = 1, loss = 2.3661062717437744, precision = 1.0\n",
            "TRAIN: ep = 33, batch = 101, loss = 2.3616483211517334, precision = 1.0\n",
            "TRAIN: ep = 33, batch = 201, loss = 2.3798346519470215, precision = 1.0\n",
            "TRAIN: ep = 33, batch = 301, loss = 2.329397678375244, precision = 1.0\n",
            "TRAIN: ep = 33, batch = 401, loss = 2.3807029724121094, precision = 1.0\n",
            "VAL: ep = 33, batch = 499, loss = 4.543386459350586, prec = 0.8399999737739563\n",
            "VAL: ep = 33, batch = 499, loss = 4.570068359375, prec = 0.7899999618530273\n",
            "VAL: ep = 33, batch = 499, loss = 3.698378324508667, prec = 0.8700000047683716\n",
            "VAL: ep = 33, batch = 499, loss = 5.352985858917236, prec = 0.75\n",
            "VAL: ep = 33, batch = 499, loss = 4.326984882354736, prec = 0.8399999737739563\n",
            "AVG: epoch: 33,\n",
            "trainloss: 0.005836836099624634, trainprecavg: 0.9998800754547119,\n",
            "validloss: 0.01135015368461609, validprecavg: 0.8093998432159424\n",
            "saved model-epoch:  33\n",
            "TRAIN: ep = 34, batch = 1, loss = 2.3336434364318848, precision = 1.0\n",
            "TRAIN: ep = 34, batch = 101, loss = 2.3721628189086914, precision = 1.0\n",
            "TRAIN: ep = 34, batch = 201, loss = 2.3459861278533936, precision = 1.0\n",
            "TRAIN: ep = 34, batch = 301, loss = 2.3167099952697754, precision = 1.0\n",
            "TRAIN: ep = 34, batch = 401, loss = 2.3840818405151367, precision = 1.0\n",
            "VAL: ep = 34, batch = 499, loss = 4.660871505737305, prec = 0.7799999713897705\n",
            "VAL: ep = 34, batch = 499, loss = 5.361159801483154, prec = 0.8100000023841858\n",
            "VAL: ep = 34, batch = 499, loss = 4.192813873291016, prec = 0.85999995470047\n",
            "VAL: ep = 34, batch = 499, loss = 4.434191703796387, prec = 0.7799999713897705\n",
            "VAL: ep = 34, batch = 499, loss = 3.745609998703003, prec = 0.8499999642372131\n",
            "AVG: epoch: 34,\n",
            "trainloss: 0.005846874713897705, trainprecavg: 0.9999200105667114,\n",
            "validloss: 0.009607816338539124, validprecavg: 0.8174000382423401\n",
            "saved model-epoch:  34\n",
            "TRAIN: ep = 35, batch = 1, loss = 2.3481976985931396, precision = 1.0\n",
            "TRAIN: ep = 35, batch = 101, loss = 2.329068183898926, precision = 1.0\n",
            "TRAIN: ep = 35, batch = 201, loss = 2.345595598220825, precision = 1.0\n",
            "TRAIN: ep = 35, batch = 301, loss = 2.359128713607788, precision = 1.0\n",
            "TRAIN: ep = 35, batch = 401, loss = 2.3219094276428223, precision = 1.0\n",
            "VAL: ep = 35, batch = 499, loss = 4.559226036071777, prec = 0.8499999642372131\n",
            "VAL: ep = 35, batch = 499, loss = 4.4671831130981445, prec = 0.8199999928474426\n",
            "VAL: ep = 35, batch = 499, loss = 3.80289888381958, prec = 0.8799999952316284\n",
            "VAL: ep = 35, batch = 499, loss = 4.910976886749268, prec = 0.7799999713897705\n",
            "VAL: ep = 35, batch = 499, loss = 4.288120269775391, prec = 0.7899999618530273\n",
            "Epoch    35: reducing learning rate of group 0 to 7.8125e-06.\n",
            "AVG: epoch: 35,\n",
            "trainloss: 0.005844148397445678, trainprecavg: 0.9999600052833557,\n",
            "validloss: 0.010309646129608155, validprecavg: 0.8114001750946045\n",
            "saved model-epoch:  35\n",
            "TRAIN: ep = 36, batch = 1, loss = 2.3524563312530518, precision = 1.0\n",
            "TRAIN: ep = 36, batch = 101, loss = 2.343919038772583, precision = 1.0\n",
            "TRAIN: ep = 36, batch = 201, loss = 2.3489925861358643, precision = 1.0\n",
            "TRAIN: ep = 36, batch = 301, loss = 2.3262100219726562, precision = 1.0\n",
            "TRAIN: ep = 36, batch = 401, loss = 2.3443727493286133, precision = 1.0\n",
            "VAL: ep = 36, batch = 499, loss = 3.91552734375, prec = 0.8499999642372131\n",
            "VAL: ep = 36, batch = 499, loss = 4.767547130584717, prec = 0.8100000023841858\n",
            "VAL: ep = 36, batch = 499, loss = 4.434391021728516, prec = 0.8399999737739563\n",
            "VAL: ep = 36, batch = 499, loss = 5.172325611114502, prec = 0.7999999523162842\n",
            "VAL: ep = 36, batch = 499, loss = 4.466678619384766, prec = 0.8499999642372131\n",
            "AVG: epoch: 36,\n",
            "trainloss: 0.005900952816009521, trainprecavg: 0.9998599886894226,\n",
            "validloss: 0.013594908714294434, validprecavg: 0.8104000687599182\n",
            "saved model-epoch:  36\n",
            "TRAIN: ep = 37, batch = 1, loss = 2.335103750228882, precision = 1.0\n",
            "TRAIN: ep = 37, batch = 101, loss = 2.330777406692505, precision = 1.0\n",
            "TRAIN: ep = 37, batch = 201, loss = 2.3587794303894043, precision = 1.0\n",
            "TRAIN: ep = 37, batch = 301, loss = 2.356424331665039, precision = 1.0\n",
            "TRAIN: ep = 37, batch = 401, loss = 2.3277230262756348, precision = 1.0\n",
            "VAL: ep = 37, batch = 499, loss = 4.576288223266602, prec = 0.7699999809265137\n",
            "VAL: ep = 37, batch = 499, loss = 4.4766974449157715, prec = 0.8999999761581421\n",
            "VAL: ep = 37, batch = 499, loss = 4.777708053588867, prec = 0.7799999713897705\n",
            "VAL: ep = 37, batch = 499, loss = 5.350342273712158, prec = 0.8199999928474426\n",
            "VAL: ep = 37, batch = 499, loss = 4.211411952972412, prec = 0.8299999833106995\n",
            "AVG: epoch: 37,\n",
            "trainloss: 0.005868686437606812, trainprecavg: 0.9999199509620667,\n",
            "validloss: 0.012031608819961548, validprecavg: 0.8103998899459839\n",
            "saved model-epoch:  37\n",
            "TRAIN: ep = 38, batch = 1, loss = 2.314955711364746, precision = 1.0\n",
            "TRAIN: ep = 38, batch = 101, loss = 2.408853530883789, precision = 1.0\n",
            "TRAIN: ep = 38, batch = 201, loss = 2.3719115257263184, precision = 1.0\n",
            "TRAIN: ep = 38, batch = 301, loss = 2.3137173652648926, precision = 1.0\n",
            "TRAIN: ep = 38, batch = 401, loss = 2.367614984512329, precision = 1.0\n",
            "VAL: ep = 38, batch = 499, loss = 5.586200714111328, prec = 0.75\n",
            "VAL: ep = 38, batch = 499, loss = 4.162269115447998, prec = 0.8299999833106995\n",
            "VAL: ep = 38, batch = 499, loss = 5.764591217041016, prec = 0.7899999618530273\n",
            "VAL: ep = 38, batch = 499, loss = 4.821325778961182, prec = 0.8100000023841858\n",
            "VAL: ep = 38, batch = 499, loss = 5.498682022094727, prec = 0.8100000023841858\n",
            "Epoch    38: reducing learning rate of group 0 to 3.9063e-06.\n",
            "AVG: epoch: 38,\n",
            "trainloss: 0.005894830226898193, trainprecavg: 1.0,\n",
            "validloss: 0.011850574016571046, validprecavg: 0.8067998290061951\n",
            "saved model-epoch:  38\n",
            "TRAIN: ep = 39, batch = 1, loss = 2.363377809524536, precision = 1.0\n",
            "TRAIN: ep = 39, batch = 101, loss = 2.3613529205322266, precision = 1.0\n",
            "TRAIN: ep = 39, batch = 201, loss = 2.3540470600128174, precision = 1.0\n",
            "TRAIN: ep = 39, batch = 301, loss = 2.4252638816833496, precision = 1.0\n",
            "TRAIN: ep = 39, batch = 401, loss = 2.3267316818237305, precision = 1.0\n",
            "VAL: ep = 39, batch = 499, loss = 5.2932634353637695, prec = 0.7799999713897705\n",
            "VAL: ep = 39, batch = 499, loss = 4.990818500518799, prec = 0.7599999904632568\n",
            "VAL: ep = 39, batch = 499, loss = 4.24653434753418, prec = 0.8199999928474426\n",
            "VAL: ep = 39, batch = 499, loss = 4.393759727478027, prec = 0.8100000023841858\n",
            "VAL: ep = 39, batch = 499, loss = 5.496971607208252, prec = 0.7599999904632568\n",
            "AVG: epoch: 39,\n",
            "trainloss: 0.006006357669830323, trainprecavg: 0.9999600052833557,\n",
            "validloss: 0.01028390645980835, validprecavg: 0.8159999847412109\n",
            "saved model-epoch:  39\n",
            "TRAIN: ep = 40, batch = 1, loss = 2.360666275024414, precision = 1.0\n",
            "TRAIN: ep = 40, batch = 101, loss = 2.3825302124023438, precision = 1.0\n",
            "TRAIN: ep = 40, batch = 201, loss = 2.3502037525177, precision = 1.0\n",
            "TRAIN: ep = 40, batch = 301, loss = 2.341996669769287, precision = 1.0\n",
            "TRAIN: ep = 40, batch = 401, loss = 2.3801207542419434, precision = 1.0\n",
            "VAL: ep = 40, batch = 499, loss = 4.765210151672363, prec = 0.8100000023841858\n",
            "VAL: ep = 40, batch = 499, loss = 4.870851516723633, prec = 0.8199999928474426\n",
            "VAL: ep = 40, batch = 499, loss = 5.484357833862305, prec = 0.8199999928474426\n",
            "VAL: ep = 40, batch = 499, loss = 4.5589728355407715, prec = 0.8399999737739563\n",
            "VAL: ep = 40, batch = 499, loss = 4.046365261077881, prec = 0.8899999856948853\n",
            "AVG: epoch: 40,\n",
            "trainloss: 0.005890995860099792, trainprecavg: 0.9999200105667114,\n",
            "validloss: 0.011805063486099244, validprecavg: 0.8082000613212585\n",
            "saved model-epoch:  40\n",
            "TRAIN: ep = 41, batch = 1, loss = 2.413391351699829, precision = 1.0\n",
            "TRAIN: ep = 41, batch = 101, loss = 2.3383800983428955, precision = 1.0\n",
            "TRAIN: ep = 41, batch = 201, loss = 2.3666555881500244, precision = 0.9899999499320984\n",
            "TRAIN: ep = 41, batch = 301, loss = 2.337265729904175, precision = 1.0\n",
            "TRAIN: ep = 41, batch = 401, loss = 2.3547604084014893, precision = 1.0\n",
            "VAL: ep = 41, batch = 499, loss = 4.5654616355896, prec = 0.8199999928474426\n",
            "VAL: ep = 41, batch = 499, loss = 3.6882314682006836, prec = 0.8799999952316284\n",
            "VAL: ep = 41, batch = 499, loss = 5.621032238006592, prec = 0.7599999904632568\n",
            "VAL: ep = 41, batch = 499, loss = 4.506935119628906, prec = 0.8499999642372131\n",
            "VAL: ep = 41, batch = 499, loss = 4.622262954711914, prec = 0.7999999523162842\n",
            "Epoch    41: reducing learning rate of group 0 to 1.9531e-06.\n",
            "AVG: epoch: 41,\n",
            "trainloss: 0.005856102108955383, trainprecavg: 0.9999400973320007,\n",
            "validloss: 0.013409798145294189, validprecavg: 0.8141999840736389\n",
            "saved model-epoch:  41\n",
            "TRAIN: ep = 42, batch = 1, loss = 2.3614134788513184, precision = 1.0\n",
            "TRAIN: ep = 42, batch = 101, loss = 2.3334341049194336, precision = 1.0\n",
            "TRAIN: ep = 42, batch = 201, loss = 2.344914436340332, precision = 1.0\n",
            "TRAIN: ep = 42, batch = 301, loss = 2.3458590507507324, precision = 1.0\n",
            "TRAIN: ep = 42, batch = 401, loss = 2.354985237121582, precision = 1.0\n",
            "VAL: ep = 42, batch = 499, loss = 5.61264705657959, prec = 0.7799999713897705\n",
            "VAL: ep = 42, batch = 499, loss = 4.395932197570801, prec = 0.8499999642372131\n",
            "VAL: ep = 42, batch = 499, loss = 6.169054985046387, prec = 0.7699999809265137\n",
            "VAL: ep = 42, batch = 499, loss = 4.529160499572754, prec = 0.85999995470047\n",
            "VAL: ep = 42, batch = 499, loss = 5.585460186004639, prec = 0.7799999713897705\n",
            "AVG: epoch: 42,\n",
            "trainloss: 0.005804648399353027, trainprecavg: 0.9999199509620667,\n",
            "validloss: 0.01063696265220642, validprecavg: 0.8089998364448547\n",
            "saved model-epoch:  42\n",
            "TRAIN: ep = 43, batch = 1, loss = 2.3425934314727783, precision = 1.0\n",
            "TRAIN: ep = 43, batch = 101, loss = 2.328843355178833, precision = 1.0\n",
            "TRAIN: ep = 43, batch = 201, loss = 2.3740131855010986, precision = 1.0\n",
            "TRAIN: ep = 43, batch = 301, loss = 2.3364760875701904, precision = 1.0\n",
            "TRAIN: ep = 43, batch = 401, loss = 2.3465399742126465, precision = 1.0\n",
            "VAL: ep = 43, batch = 499, loss = 4.692324161529541, prec = 0.8299999833106995\n",
            "VAL: ep = 43, batch = 499, loss = 5.336661338806152, prec = 0.8399999737739563\n",
            "VAL: ep = 43, batch = 499, loss = 4.104489803314209, prec = 0.8700000047683716\n",
            "VAL: ep = 43, batch = 499, loss = 4.627405166625977, prec = 0.85999995470047\n",
            "VAL: ep = 43, batch = 499, loss = 4.585052013397217, prec = 0.8199999928474426\n",
            "AVG: epoch: 43,\n",
            "trainloss: 0.005827840566635132, trainprecavg: 0.9999800324440002,\n",
            "validloss: 0.011090224981307984, validprecavg: 0.8128000497817993\n",
            "saved model-epoch:  43\n",
            "TRAIN: ep = 44, batch = 1, loss = 2.3254032135009766, precision = 1.0\n",
            "TRAIN: ep = 44, batch = 101, loss = 2.3444197177886963, precision = 1.0\n",
            "TRAIN: ep = 44, batch = 201, loss = 2.33463978767395, precision = 1.0\n",
            "TRAIN: ep = 44, batch = 301, loss = 2.339696168899536, precision = 1.0\n",
            "TRAIN: ep = 44, batch = 401, loss = 2.3779189586639404, precision = 1.0\n",
            "VAL: ep = 44, batch = 499, loss = 4.137495517730713, prec = 0.8499999642372131\n",
            "VAL: ep = 44, batch = 499, loss = 4.612700939178467, prec = 0.8100000023841858\n",
            "VAL: ep = 44, batch = 499, loss = 4.782306671142578, prec = 0.7799999713897705\n",
            "VAL: ep = 44, batch = 499, loss = 4.700323581695557, prec = 0.7899999618530273\n",
            "VAL: ep = 44, batch = 499, loss = 4.062868595123291, prec = 0.8100000023841858\n",
            "Epoch    44: reducing learning rate of group 0 to 9.7656e-07.\n",
            "AVG: epoch: 44,\n",
            "trainloss: 0.005889552235603332, trainprecavg: 0.9999600052833557,\n",
            "validloss: 0.013031343221664429, validprecavg: 0.8072000741958618\n",
            "saved model-epoch:  44\n",
            "TRAIN: ep = 45, batch = 1, loss = 2.39103627204895, precision = 1.0\n",
            "TRAIN: ep = 45, batch = 101, loss = 2.3780815601348877, precision = 1.0\n",
            "TRAIN: ep = 45, batch = 201, loss = 2.3483693599700928, precision = 1.0\n",
            "TRAIN: ep = 45, batch = 301, loss = 2.3268091678619385, precision = 1.0\n",
            "TRAIN: ep = 45, batch = 401, loss = 2.3225927352905273, precision = 1.0\n",
            "VAL: ep = 45, batch = 499, loss = 4.848552703857422, prec = 0.75\n",
            "VAL: ep = 45, batch = 499, loss = 4.814388275146484, prec = 0.7899999618530273\n",
            "VAL: ep = 45, batch = 499, loss = 4.9927287101745605, prec = 0.8100000023841858\n",
            "VAL: ep = 45, batch = 499, loss = 5.3618388175964355, prec = 0.7699999809265137\n",
            "VAL: ep = 45, batch = 499, loss = 4.501317501068115, prec = 0.8299999833106995\n",
            "AVG: epoch: 45,\n",
            "trainloss: 0.005844390392303467, trainprecavg: 0.9998800158500671,\n",
            "validloss: 0.012766947746276855, validprecavg: 0.8028001189231873\n",
            "saved model-epoch:  45\n",
            "TRAIN: ep = 46, batch = 1, loss = 2.3308475017547607, precision = 1.0\n",
            "TRAIN: ep = 46, batch = 101, loss = 2.3610215187072754, precision = 1.0\n",
            "TRAIN: ep = 46, batch = 201, loss = 2.337862968444824, precision = 1.0\n",
            "TRAIN: ep = 46, batch = 301, loss = 2.3830697536468506, precision = 1.0\n",
            "TRAIN: ep = 46, batch = 401, loss = 2.3560478687286377, precision = 1.0\n",
            "VAL: ep = 46, batch = 499, loss = 5.319194793701172, prec = 0.7599999904632568\n",
            "VAL: ep = 46, batch = 499, loss = 5.2056193351745605, prec = 0.8199999928474426\n",
            "VAL: ep = 46, batch = 499, loss = 6.053140640258789, prec = 0.7599999904632568\n",
            "VAL: ep = 46, batch = 499, loss = 3.893202543258667, prec = 0.8799999952316284\n",
            "VAL: ep = 46, batch = 499, loss = 5.6489949226379395, prec = 0.7699999809265137\n",
            "AVG: epoch: 46,\n",
            "trainloss: 0.005828951001167298, trainprecavg: 0.9998600482940674,\n",
            "validloss: 0.011764168739318848, validprecavg: 0.8091999888420105\n",
            "saved model-epoch:  46\n",
            "TRAIN: ep = 47, batch = 1, loss = 2.3780224323272705, precision = 1.0\n",
            "TRAIN: ep = 47, batch = 101, loss = 2.3531877994537354, precision = 1.0\n",
            "TRAIN: ep = 47, batch = 201, loss = 2.3429057598114014, precision = 1.0\n",
            "TRAIN: ep = 47, batch = 301, loss = 2.3481361865997314, precision = 1.0\n",
            "TRAIN: ep = 47, batch = 401, loss = 2.3277008533477783, precision = 1.0\n",
            "VAL: ep = 47, batch = 499, loss = 3.872664213180542, prec = 0.85999995470047\n",
            "VAL: ep = 47, batch = 499, loss = 5.100845813751221, prec = 0.7400000095367432\n",
            "VAL: ep = 47, batch = 499, loss = 5.504222393035889, prec = 0.7999999523162842\n",
            "VAL: ep = 47, batch = 499, loss = 4.334573745727539, prec = 0.8299999833106995\n",
            "VAL: ep = 47, batch = 499, loss = 5.091423511505127, prec = 0.8399999737739563\n",
            "Epoch    47: reducing learning rate of group 0 to 4.8828e-07.\n",
            "AVG: epoch: 47,\n",
            "trainloss: 0.005806940793991089, trainprecavg: 0.999940037727356,\n",
            "validloss: 0.01153059720993042, validprecavg: 0.8191999793052673\n",
            "saved model-epoch:  47\n",
            "TRAIN: ep = 48, batch = 1, loss = 2.3936963081359863, precision = 1.0\n",
            "TRAIN: ep = 48, batch = 101, loss = 2.326911687850952, precision = 1.0\n",
            "TRAIN: ep = 48, batch = 201, loss = 2.3414087295532227, precision = 1.0\n",
            "TRAIN: ep = 48, batch = 301, loss = 2.3428385257720947, precision = 1.0\n",
            "TRAIN: ep = 48, batch = 401, loss = 2.37341046333313, precision = 1.0\n",
            "VAL: ep = 48, batch = 499, loss = 5.134359836578369, prec = 0.7699999809265137\n",
            "VAL: ep = 48, batch = 499, loss = 5.31356143951416, prec = 0.8499999642372131\n",
            "VAL: ep = 48, batch = 499, loss = 4.319349765777588, prec = 0.8399999737739563\n",
            "VAL: ep = 48, batch = 499, loss = 5.78476619720459, prec = 0.7599999904632568\n",
            "VAL: ep = 48, batch = 499, loss = 4.938465595245361, prec = 0.8100000023841858\n",
            "AVG: epoch: 48,\n",
            "trainloss: 0.005902819037437439, trainprecavg: 0.9999399781227112,\n",
            "validloss: 0.013098400831222535, validprecavg: 0.8122000694274902\n",
            "saved model-epoch:  48\n",
            "TRAIN: ep = 49, batch = 1, loss = 2.3803741931915283, precision = 1.0\n",
            "TRAIN: ep = 49, batch = 101, loss = 2.31294322013855, precision = 1.0\n",
            "TRAIN: ep = 49, batch = 201, loss = 2.3459672927856445, precision = 1.0\n",
            "TRAIN: ep = 49, batch = 301, loss = 2.3307290077209473, precision = 1.0\n",
            "TRAIN: ep = 49, batch = 401, loss = 2.384070873260498, precision = 1.0\n",
            "VAL: ep = 49, batch = 499, loss = 5.009943962097168, prec = 0.8299999833106995\n",
            "VAL: ep = 49, batch = 499, loss = 5.6866583824157715, prec = 0.7400000095367432\n",
            "VAL: ep = 49, batch = 499, loss = 4.206794261932373, prec = 0.8299999833106995\n",
            "VAL: ep = 49, batch = 499, loss = 3.6375491619110107, prec = 0.8700000047683716\n",
            "VAL: ep = 49, batch = 499, loss = 4.831892490386963, prec = 0.8199999928474426\n",
            "AVG: epoch: 49,\n",
            "trainloss: 0.005903204083442688, trainprecavg: 0.9999600052833557,\n",
            "validloss: 0.011105886697769164, validprecavg: 0.8150002360343933\n",
            "saved model-epoch:  49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpL8MwlCmuZG",
        "colab_type": "code",
        "outputId": "eeba5094-8085-4b80-e33b-ef6aea19e566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1970
        }
      },
      "source": [
        "#cf10_test = CIFAR10('./', train=False, transform=ds_transform,  download=True)\n",
        "#test_loader = get_loader(cf10_test, shuffle=True)\n",
        "\n",
        "testmodel = SimpleCNN(len(idx2label)).to(device)\n",
        "for i in range(47, 48):\n",
        "  load_ep=i\n",
        "\n",
        "  testmodel.load_state_dict(torch.load('./solaris_SimpleCNN-'+str(load_ep)+'.pth'))\n",
        "  testmodel.eval()\n",
        "  listprec= []\n",
        "  for i in range(10):\n",
        "    sum_prec = 0\n",
        "    for vbatch, (b_img_t, b_label) in enumerate(test_loader):\n",
        "      if vbatch<50: continue\n",
        "      b_img_t = b_img_t.to(device)\n",
        "      b_label = b_label.to(device)\n",
        "\n",
        "      pred = testmodel(b_img_t).argmax(dim=1)\n",
        "      b_prec = (pred==b_label).float().sum()/len(b_label) # len(b_label == batchlen)\n",
        "      sum_prec+=b_prec\n",
        "\n",
        "    testset_prec = 2*sum_prec/len(test_loader)\n",
        "    listprec.append(testset_prec)\n",
        "    print(testset_prec)\n",
        "    print(\"answers:\", b_label, \"\\npredictions:\", pred)\n",
        "  print(load_ep, (torch.stack(listprec).flatten().sum()/10).item())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8100, device='cuda:0')\n",
            "answers: tensor([1, 8, 9, 8, 7, 8, 4, 1, 3, 6, 9, 8, 5, 4, 2, 4, 1, 5, 6, 6, 6, 9, 2, 8,\n",
            "        7, 1, 9, 1, 9, 1, 2, 8, 9, 2, 1, 3, 0, 6, 8, 9, 5, 1, 5, 3, 8, 7, 2, 6,\n",
            "        0, 6, 6, 2, 4, 4, 9, 4, 0, 0, 6, 5, 0, 7, 2, 0, 6, 0, 4, 0, 2, 5, 9, 6,\n",
            "        6, 6, 4, 0, 2, 1, 4, 4, 6, 8, 0, 8, 6, 1, 6, 4, 1, 7, 2, 1, 9, 5, 2, 3,\n",
            "        0, 0, 2, 3], device='cuda:0') \n",
            "predictions: tensor([1, 8, 9, 8, 2, 8, 4, 1, 7, 6, 9, 8, 5, 6, 2, 4, 1, 2, 6, 6, 2, 9, 7, 8,\n",
            "        7, 1, 9, 1, 3, 1, 7, 8, 9, 2, 1, 3, 0, 6, 8, 9, 5, 1, 5, 3, 8, 7, 2, 6,\n",
            "        0, 6, 6, 9, 4, 4, 9, 4, 0, 0, 4, 3, 8, 7, 2, 0, 6, 0, 4, 8, 2, 5, 8, 6,\n",
            "        4, 6, 4, 0, 2, 1, 4, 4, 6, 5, 0, 8, 4, 1, 3, 4, 1, 7, 0, 1, 9, 5, 2, 3,\n",
            "        0, 0, 5, 4], device='cuda:0')\n",
            "tensor(0.8094, device='cuda:0')\n",
            "answers: tensor([3, 8, 4, 0, 9, 8, 3, 1, 8, 8, 3, 2, 0, 6, 9, 7, 2, 9, 6, 5, 3, 7, 2, 0,\n",
            "        0, 5, 8, 4, 2, 8, 6, 4, 1, 6, 3, 1, 9, 8, 8, 6, 2, 9, 3, 9, 2, 4, 7, 9,\n",
            "        1, 8, 7, 6, 3, 6, 2, 9, 3, 7, 3, 9, 6, 9, 0, 2, 5, 5, 5, 8, 0, 9, 3, 0,\n",
            "        9, 6, 0, 3, 9, 9, 1, 1, 4, 5, 4, 0, 9, 6, 3, 5, 1, 5, 8, 3, 6, 5, 8, 3,\n",
            "        6, 5, 1, 4], device='cuda:0') \n",
            "predictions: tensor([6, 8, 4, 0, 9, 8, 3, 9, 0, 8, 2, 2, 0, 8, 9, 7, 2, 9, 6, 5, 3, 7, 2, 0,\n",
            "        0, 2, 8, 4, 2, 8, 6, 4, 1, 6, 3, 1, 9, 8, 9, 6, 4, 9, 3, 9, 2, 4, 7, 9,\n",
            "        1, 8, 7, 6, 9, 6, 2, 9, 3, 2, 5, 9, 6, 1, 0, 2, 5, 5, 5, 2, 0, 9, 3, 0,\n",
            "        9, 6, 8, 3, 9, 9, 1, 8, 3, 5, 4, 0, 9, 3, 5, 5, 1, 5, 8, 3, 6, 5, 8, 3,\n",
            "        6, 5, 1, 4], device='cuda:0')\n",
            "tensor(0.8092, device='cuda:0')\n",
            "answers: tensor([7, 4, 3, 7, 1, 6, 4, 9, 8, 5, 9, 1, 5, 9, 6, 1, 5, 2, 6, 3, 4, 0, 1, 7,\n",
            "        9, 0, 6, 3, 6, 6, 1, 0, 3, 5, 8, 7, 5, 9, 5, 6, 4, 0, 5, 9, 9, 7, 5, 4,\n",
            "        0, 2, 8, 3, 2, 6, 7, 7, 0, 1, 1, 7, 2, 2, 2, 3, 1, 7, 6, 7, 1, 8, 8, 7,\n",
            "        0, 2, 9, 5, 8, 9, 7, 7, 6, 1, 9, 4, 0, 3, 1, 1, 3, 6, 9, 5, 4, 9, 4, 3,\n",
            "        0, 2, 0, 4], device='cuda:0') \n",
            "predictions: tensor([5, 4, 3, 4, 1, 6, 4, 9, 8, 3, 9, 1, 5, 9, 6, 1, 5, 2, 6, 4, 4, 8, 1, 7,\n",
            "        9, 8, 6, 3, 6, 6, 1, 0, 3, 5, 8, 7, 5, 9, 5, 6, 4, 0, 8, 9, 9, 7, 5, 4,\n",
            "        0, 2, 8, 5, 2, 6, 7, 7, 1, 1, 1, 7, 2, 2, 2, 3, 1, 7, 6, 7, 1, 8, 8, 7,\n",
            "        0, 2, 9, 5, 8, 9, 7, 7, 6, 1, 9, 7, 0, 3, 1, 1, 5, 4, 9, 3, 7, 9, 4, 7,\n",
            "        2, 2, 0, 4], device='cuda:0')\n",
            "tensor(0.8092, device='cuda:0')\n",
            "answers: tensor([9, 0, 8, 3, 4, 8, 4, 5, 5, 7, 4, 4, 5, 3, 7, 2, 1, 8, 6, 3, 9, 0, 9, 0,\n",
            "        0, 0, 9, 3, 0, 3, 4, 8, 0, 0, 7, 1, 9, 1, 3, 1, 1, 7, 6, 7, 0, 9, 5, 2,\n",
            "        3, 5, 4, 5, 0, 2, 4, 8, 0, 1, 5, 1, 3, 9, 4, 2, 2, 2, 4, 9, 9, 5, 2, 9,\n",
            "        7, 6, 4, 7, 6, 0, 3, 6, 7, 7, 8, 6, 9, 6, 3, 3, 4, 7, 1, 5, 7, 7, 6, 1,\n",
            "        4, 4, 1, 9], device='cuda:0') \n",
            "predictions: tensor([9, 0, 8, 3, 4, 7, 4, 7, 5, 7, 4, 4, 5, 3, 7, 5, 9, 8, 6, 3, 9, 0, 9, 0,\n",
            "        0, 0, 9, 3, 9, 3, 4, 8, 0, 0, 7, 1, 5, 1, 3, 1, 1, 7, 2, 7, 0, 9, 5, 2,\n",
            "        3, 5, 2, 5, 0, 2, 7, 8, 0, 1, 5, 1, 3, 9, 4, 2, 2, 2, 4, 9, 9, 6, 2, 9,\n",
            "        7, 6, 2, 7, 6, 0, 3, 6, 7, 7, 8, 6, 9, 6, 3, 2, 4, 7, 1, 5, 7, 7, 6, 1,\n",
            "        6, 2, 1, 9], device='cuda:0')\n",
            "tensor(0.8134, device='cuda:0')\n",
            "answers: tensor([3, 0, 8, 6, 8, 8, 2, 6, 4, 2, 1, 6, 5, 4, 5, 1, 2, 4, 2, 4, 4, 8, 3, 7,\n",
            "        2, 0, 2, 5, 1, 4, 5, 2, 1, 3, 8, 0, 3, 4, 0, 4, 8, 2, 0, 9, 0, 3, 7, 0,\n",
            "        2, 5, 8, 2, 1, 1, 6, 8, 6, 6, 8, 7, 0, 9, 5, 4, 2, 4, 6, 2, 7, 4, 3, 9,\n",
            "        6, 4, 0, 9, 6, 4, 5, 1, 2, 2, 1, 6, 4, 6, 9, 3, 1, 2, 7, 2, 4, 3, 9, 9,\n",
            "        3, 7, 6, 0], device='cuda:0') \n",
            "predictions: tensor([2, 0, 8, 6, 8, 8, 3, 6, 0, 2, 1, 6, 5, 4, 5, 1, 2, 4, 2, 4, 4, 8, 3, 7,\n",
            "        2, 0, 2, 5, 1, 6, 5, 2, 1, 3, 8, 0, 3, 4, 0, 4, 8, 3, 0, 9, 0, 3, 7, 0,\n",
            "        2, 5, 8, 3, 1, 8, 6, 8, 6, 4, 8, 7, 0, 9, 5, 4, 3, 4, 3, 2, 7, 4, 5, 9,\n",
            "        6, 4, 0, 9, 6, 4, 0, 1, 2, 2, 1, 6, 0, 6, 9, 2, 1, 5, 7, 0, 3, 5, 9, 1,\n",
            "        3, 7, 6, 0], device='cuda:0')\n",
            "tensor(0.8210, device='cuda:0')\n",
            "answers: tensor([8, 5, 0, 0, 5, 0, 6, 9, 4, 3, 4, 6, 1, 8, 2, 7, 4, 8, 4, 8, 0, 2, 6, 7,\n",
            "        4, 8, 9, 8, 4, 8, 8, 0, 8, 3, 5, 2, 8, 7, 2, 0, 7, 8, 2, 9, 1, 8, 1, 8,\n",
            "        5, 4, 7, 2, 8, 3, 3, 1, 4, 1, 8, 7, 3, 6, 3, 8, 9, 3, 3, 9, 8, 2, 2, 9,\n",
            "        8, 1, 8, 5, 4, 8, 1, 5, 2, 3, 8, 2, 8, 1, 9, 9, 3, 8, 2, 7, 4, 2, 6, 6,\n",
            "        0, 3, 7, 8], device='cuda:0') \n",
            "predictions: tensor([8, 5, 0, 0, 7, 9, 6, 9, 4, 4, 4, 6, 1, 8, 2, 7, 4, 8, 3, 8, 0, 2, 6, 7,\n",
            "        4, 8, 1, 8, 4, 8, 8, 0, 8, 4, 3, 4, 8, 7, 0, 0, 7, 0, 2, 9, 1, 8, 1, 5,\n",
            "        3, 2, 3, 5, 8, 3, 3, 1, 4, 1, 8, 7, 0, 6, 3, 1, 9, 5, 3, 9, 8, 2, 2, 9,\n",
            "        8, 9, 8, 5, 4, 8, 1, 5, 5, 3, 2, 3, 8, 1, 9, 2, 6, 8, 4, 7, 4, 7, 6, 6,\n",
            "        0, 3, 0, 3], device='cuda:0')\n",
            "tensor(0.8114, device='cuda:0')\n",
            "answers: tensor([0, 9, 2, 1, 5, 2, 8, 5, 1, 1, 8, 7, 4, 1, 7, 3, 1, 4, 0, 2, 2, 6, 7, 2,\n",
            "        7, 9, 3, 6, 3, 3, 0, 1, 6, 8, 0, 8, 0, 3, 3, 7, 6, 8, 2, 2, 1, 9, 0, 8,\n",
            "        2, 4, 7, 7, 2, 3, 0, 3, 6, 0, 7, 9, 9, 9, 8, 2, 9, 4, 5, 7, 9, 5, 5, 1,\n",
            "        0, 2, 3, 0, 7, 7, 5, 3, 0, 9, 8, 3, 1, 8, 5, 1, 9, 5, 4, 4, 1, 8, 0, 9,\n",
            "        2, 4, 4, 5], device='cuda:0') \n",
            "predictions: tensor([0, 9, 4, 1, 5, 3, 8, 2, 1, 1, 8, 7, 5, 1, 7, 3, 6, 4, 0, 2, 2, 6, 7, 2,\n",
            "        7, 9, 3, 6, 3, 3, 8, 1, 6, 8, 0, 8, 0, 7, 3, 7, 6, 8, 2, 2, 1, 9, 0, 8,\n",
            "        0, 4, 4, 7, 0, 3, 0, 3, 6, 2, 7, 9, 9, 9, 8, 7, 9, 3, 5, 7, 0, 5, 5, 1,\n",
            "        0, 2, 3, 9, 7, 2, 5, 5, 8, 9, 8, 3, 1, 8, 5, 1, 9, 5, 3, 4, 1, 8, 0, 9,\n",
            "        5, 4, 4, 3], device='cuda:0')\n",
            "tensor(0.8122, device='cuda:0')\n",
            "answers: tensor([4, 8, 6, 5, 4, 6, 0, 9, 8, 6, 3, 8, 2, 9, 8, 3, 3, 5, 6, 5, 3, 7, 4, 7,\n",
            "        9, 5, 0, 5, 0, 1, 0, 2, 7, 4, 2, 6, 2, 4, 2, 0, 0, 0, 3, 9, 0, 9, 0, 2,\n",
            "        0, 6, 9, 0, 5, 2, 0, 8, 9, 9, 8, 9, 4, 4, 4, 9, 8, 5, 6, 5, 0, 7, 8, 1,\n",
            "        4, 5, 5, 3, 1, 9, 0, 7, 9, 0, 0, 5, 2, 7, 0, 6, 8, 9, 2, 7, 9, 1, 1, 5,\n",
            "        5, 0, 2, 0], device='cuda:0') \n",
            "predictions: tensor([4, 8, 6, 5, 4, 6, 7, 9, 8, 6, 3, 8, 2, 9, 3, 5, 5, 5, 6, 5, 3, 5, 2, 7,\n",
            "        9, 3, 0, 5, 0, 1, 2, 2, 7, 4, 2, 6, 2, 4, 2, 0, 0, 8, 5, 9, 0, 9, 0, 2,\n",
            "        0, 6, 9, 0, 5, 2, 0, 8, 9, 9, 8, 9, 4, 4, 4, 9, 8, 5, 3, 5, 0, 7, 8, 1,\n",
            "        4, 5, 5, 5, 1, 9, 0, 7, 9, 2, 1, 5, 2, 7, 0, 3, 8, 9, 2, 7, 9, 1, 1, 5,\n",
            "        3, 0, 2, 0], device='cuda:0')\n",
            "tensor(0.8108, device='cuda:0')\n",
            "answers: tensor([9, 5, 0, 0, 2, 5, 9, 3, 2, 1, 4, 1, 0, 9, 9, 7, 7, 9, 8, 7, 1, 5, 8, 4,\n",
            "        7, 9, 4, 9, 2, 2, 0, 9, 9, 6, 2, 6, 0, 6, 2, 9, 7, 1, 4, 9, 8, 8, 8, 9,\n",
            "        3, 7, 9, 9, 6, 4, 7, 7, 7, 0, 2, 1, 2, 9, 2, 3, 5, 2, 0, 5, 9, 5, 8, 6,\n",
            "        3, 9, 4, 8, 8, 2, 8, 0, 8, 5, 6, 1, 7, 2, 1, 7, 9, 5, 6, 8, 8, 0, 6, 9,\n",
            "        5, 8, 8, 8], device='cuda:0') \n",
            "predictions: tensor([8, 4, 0, 0, 2, 5, 9, 0, 2, 1, 4, 1, 0, 9, 9, 7, 7, 9, 8, 7, 8, 5, 8, 4,\n",
            "        7, 9, 4, 9, 2, 2, 0, 9, 9, 6, 2, 6, 0, 6, 2, 1, 7, 1, 4, 1, 8, 8, 8, 9,\n",
            "        3, 7, 5, 9, 3, 4, 7, 7, 7, 0, 2, 9, 2, 9, 2, 3, 5, 2, 0, 5, 9, 3, 7, 6,\n",
            "        3, 9, 4, 8, 8, 9, 8, 0, 8, 2, 6, 1, 7, 0, 1, 7, 9, 5, 6, 8, 8, 0, 6, 9,\n",
            "        5, 8, 8, 8], device='cuda:0')\n",
            "tensor(0.8108, device='cuda:0')\n",
            "answers: tensor([6, 2, 2, 1, 2, 3, 4, 9, 6, 0, 3, 5, 8, 4, 1, 3, 4, 6, 6, 9, 6, 3, 4, 6,\n",
            "        1, 6, 0, 5, 3, 8, 3, 6, 3, 7, 2, 0, 9, 3, 5, 7, 0, 8, 0, 9, 8, 9, 6, 3,\n",
            "        5, 8, 5, 1, 9, 9, 1, 6, 0, 4, 9, 7, 0, 0, 1, 6, 0, 3, 2, 8, 5, 9, 7, 8,\n",
            "        0, 0, 3, 5, 9, 6, 3, 7, 8, 7, 7, 1, 2, 4, 2, 7, 2, 3, 1, 7, 6, 7, 9, 0,\n",
            "        6, 8, 4, 9], device='cuda:0') \n",
            "predictions: tensor([2, 2, 0, 1, 2, 0, 4, 2, 6, 0, 3, 7, 8, 4, 1, 3, 4, 6, 2, 9, 4, 3, 4, 6,\n",
            "        1, 6, 0, 2, 4, 8, 4, 6, 5, 7, 2, 0, 9, 3, 7, 7, 0, 8, 0, 9, 2, 9, 6, 4,\n",
            "        6, 8, 5, 1, 9, 9, 1, 3, 0, 4, 9, 7, 0, 0, 1, 6, 0, 3, 2, 8, 5, 9, 7, 8,\n",
            "        2, 0, 3, 5, 3, 6, 7, 7, 8, 7, 7, 1, 2, 4, 2, 7, 2, 5, 1, 7, 6, 3, 3, 0,\n",
            "        6, 8, 4, 1], device='cuda:0')\n",
            "47 0.8117400407791138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94hT78DYa8ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BahMCcKqbH9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRdx6c6JbTe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}